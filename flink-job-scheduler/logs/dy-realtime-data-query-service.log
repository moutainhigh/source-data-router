[2019-02-27 17:47:49.815] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 13088 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 17:47:49.817] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: prod
[2019-02-27 17:47:53.838] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 17:47:54.121] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 17:49:02.538] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 74.085 seconds (JVM running for 205.504)
[2019-02-27 17:49:04.615] [threadPoolTaskScheduler-1] [WARN ] org.apache.hadoop.util.ShutdownHookManager - Failed to add the ShutdownHook
java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.add(ApplicationShutdownHooks.java:66)
	at java.lang.Runtime.addShutdownHook(Runtime.java:211)
	at org.apache.hadoop.util.ShutdownHookManager.<clinit>(ShutdownHookManager.java:59)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2858)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181)
	at com.globalegrow.hdfs.utils.HdfsUtil.getDyActiceService(HdfsUtil.java:31)
	at com.globalegrow.fixed.scheduler.ApacheDrillHdfsActiveNameNodeCheck.run(ApacheDrillHdfsActiveNameNodeCheck.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:65)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-27 17:49:04.643] [threadPoolTaskScheduler-1] [WARN ] org.apache.hadoop.ipc.Client - Failed to connect to server: bts-master/52.20.83.154:8020: try once and fail.
java.nio.channels.ClosedByInterruptException: null
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy88.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy89.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436)
	at org.apache.hadoop.hdfs.HAUtil.getAddressOfActive(HAUtil.java:315)
	at com.globalegrow.hdfs.utils.HdfsUtil.getDyActiceService(HdfsUtil.java:32)
	at com.globalegrow.fixed.scheduler.ApacheDrillHdfsActiveNameNodeCheck.run(ApacheDrillHdfsActiveNameNodeCheck.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:65)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-27 17:49:04.660] [threadPoolTaskScheduler-1] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - 大禹 hdfs 主备切换，由 hdfs://bts-masterbak:8020 切换为 ， 更新 Apache drill 配置
[2019-02-27 17:49:05.379] [threadPoolTaskScheduler-1] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - 更新结果 200
[2019-02-27 17:49:09.971] [threadPoolTaskScheduler-1] [WARN ] org.apache.hadoop.ipc.Client - Failed to connect to server: 172.31.20.96/172.31.20.96:8020: try once and fail.
java.nio.channels.ClosedByInterruptException: null
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:659)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy88.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy89.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436)
	at org.apache.hadoop.hdfs.HAUtil.getAddressOfActive(HAUtil.java:315)
	at com.globalegrow.hdfs.utils.HdfsUtil.getBigDataActiveNamenode(HdfsUtil.java:48)
	at com.globalegrow.fixed.scheduler.ApacheDrillHdfsActiveNameNodeCheck.run(ApacheDrillHdfsActiveNameNodeCheck.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:65)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2019-02-27 17:49:09.979] [threadPoolTaskScheduler-1] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - 大数据 hdfs 主备切换，由 hdfs://172.31.20.96:8020 切换为 ， 更新 Apache drill 配置
[2019-02-27 17:49:10.452] [threadPoolTaskScheduler-1] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - 更新结果 200
[2019-02-27 17:53:40.818] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 4548 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 17:53:40.824] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: prod
[2019-02-27 17:53:44.009] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 17:53:44.233] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 17:55:32.984] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 112.676 seconds (JVM running for 114.093)
[2019-02-27 17:56:26.446] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 6484 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 17:56:26.447] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 17:56:28.741] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 17:56:28.964] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:02:18.599] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 13352 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:02:18.601] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:02:22.542] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:02:22.769] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:09:49.390] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 12524 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:09:49.391] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:09:52.171] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:09:52.401] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:10:02.471] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 13.735 seconds (JVM running for 15.24)
[2019-02-27 18:10:26.209] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 10748 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:10:26.211] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:10:28.824] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:10:29.043] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:10:39.074] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 13.547 seconds (JVM running for 14.992)
[2019-02-27 18:10:53.450] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 15140 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:10:53.451] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:10:55.801] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:10:56.028] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:11:05.825] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 12.888 seconds (JVM running for 14.326)
[2019-02-27 18:11:40.031] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 17084 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:11:40.032] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:11:42.418] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:11:42.637] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:11:52.312] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 12.977 seconds (JVM running for 14.344)
[2019-02-27 18:12:23.859] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 17192 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:12:23.860] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:12:26.398] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:12:26.622] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:12:36.330] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 12.915 seconds (JVM running for 14.306)
[2019-02-27 18:12:57.822] [main] [INFO ] com.globalegrow.EsTest - Starting EsTest on 3F-wangzhongfu with PID 13544 (started by wangzhongfu in E:\work\globale\git\source-data-router\flink-job-scheduler)
[2019-02-27 18:12:57.830] [main] [INFO ] com.globalegrow.EsTest - The following profiles are active: dev
[2019-02-27 18:13:00.431] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大禹 dfs 配置信息 {name=dfs, config={type=file, connection=hdfs://bts-masterbak:8020, config=null, workspaces={zaful={location=/user/hadoop/bumblebee/web/zaful, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, tmp={location=/tmp, writable=true, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, root={location=/, writable=false, defaultInputFormat=null, allowAccessOutsideWorkspace=false}, hadoop={location=/user/hadoop/log/m/zaful/clean/, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}, bumblebee={location=/user/hadoop/bumblebee/, writable=false, defaultInputFormat=avro, allowAccessOutsideWorkspace=true}, json_test={location=/user/hadoop/apache_drill_json_test, writable=false, defaultInputFormat=json, allowAccessOutsideWorkspace=true}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=,}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:13:00.657] [main] [INFO ] c.g.f.scheduler.ApacheDrillHdfsActiveNameNodeCheck - Apache drill 大数据 dfs 配置信息 {name=dfs_bigdata, config={type=file, connection=hdfs://172.31.20.96:8020, config=null, workspaces={bigdata={location=/bigdata/ods/, writable=false, defaultInputFormat=csv, allowAccessOutsideWorkspace=false}}, formats={psv={type=text, extensions=[tbl], delimiter=|}, csv={type=text, extensions=[csv], delimiter=}, tsv={type=text, extensions=[tsv], delimiter=	}, httpd={type=httpd, logFormat=%h %t "%r" %>s %b "%{Referer}i"}, parquet={type=parquet}, json={type=json, extensions=[json]}, pcap={type=pcap}, pcapng={type=pcapng, extensions=[pcapng]}, avro={type=avro}, sequencefile={type=sequencefile, extensions=[seq]}, csvh={type=text, extensions=[csvh], extractHeader=true, delimiter=,}, image={type=image, extensions=[jpg, jpeg, jpe, tif, tiff, dng, psd, png, bmp, gif, ico, pcx, wav, wave, avi, webp, mov, mp4, m4a, m4p, m4b, m4r, m4v, 3gp, 3g2, eps, epsf, epsi, ai, arw, crw, cr2, nef, orf, raf, rw2, rwl, srw, x3f]}}, enabled=true}}
[2019-02-27 18:13:10.432] [main] [INFO ] com.globalegrow.EsTest - Started EsTest in 13.226 seconds (JVM running for 15.054)
